'''
This file is for validating regressed electricity demand data from resstock and comstock. The "ref" data is aggregated electricity demand directly from resstock/comstock for a given weather year, while the "reg" data is regressed data for the same weather year, generated by geo_predict, which is trained on resstock/comstock data (and associated weather data) from a different weather year.

This script was tested with the reeds2 conda environment.
'''

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from pathlib import Path

#Input the regression and reference file paths here.
REF_PATH = Path(r"C:\ReEDS\geo_predict\validation_2025-12-19\com_0_ref_outputs_2025-12-10-15-53-30\agg_com_eulp_hvac_elec_GWh_upgrade0.csv")
REG_PATH = Path(r"C:\ReEDS\geo_predict\validation_2025-12-19\com_0_reg_outputs_2025-12-02-09-29-48\agg_com_eulp_hvac_elec_GWh_upgrade0.csv")
OUTPUT_DIR = Path("validation_outputs")
COUNTY_MAP_URL = "https://raw.githubusercontent.com/NREL/ReEDS-2.0/refs/heads/main/inputs/county2zone.csv"

SEASON_LOOKUP = {
    12: "DJF",
    1: "DJF",
    2: "DJF",
    3: "MAM",
    4: "MAM",
    5: "MAM",
    6: "JJA",
    7: "JJA",
    8: "JJA",
    9: "SON",
    10: "SON",
    11: "SON",
}
SEASON_ORDER = ["DJF", "MAM", "JJA", "SON"]
PCTL = [90, 95, 99]


def normalize_fips(val) -> str:
    sval = str(val).strip()
    sval = sval.lstrip("0")
    return sval or "0"


def load_county_metadata(counties):
    try:
        mapping = pd.read_csv(COUNTY_MAP_URL, dtype={"FIPS": str})
    except Exception as exc:
        raise SystemExit(f"Failed to load county mapping from {COUNTY_MAP_URL}: {exc}")
    mapping["FIPS"] = mapping["FIPS"].astype(str).str.strip()
    mapping["ba"] = mapping["ba"].astype(str).str.strip()
    mapping["fips_norm"] = mapping["FIPS"].apply(normalize_fips)
    mapping["county_name"] = mapping["county_name"].astype(str).str.strip()
    mapping["state"] = mapping["state"].astype(str).str.strip()
    subset = mapping[mapping["fips_norm"].isin(counties)].drop_duplicates(subset=["fips_norm"]).set_index("fips_norm")
    missing = sorted(set(counties) - set(subset.index))
    return subset, missing


def county_label(fips: str, county_meta: pd.DataFrame):
    if fips in county_meta.index:
        row = county_meta.loc[fips]
        name = row["county_name"].title()
        state = row["state"]
        return f"{name} County, {state} ({fips})"
    return f"FIPS {fips}"


def ba_label(ba: str, ba_states: dict):
    state = ba_states.get(ba)
    return f"BA {ba} ({state})" if state else f"BA {ba}"


def ba_state_lookup_from_meta(county_meta: pd.DataFrame):
    grouped = county_meta.groupby("ba")["state"].agg(lambda s: sorted(set(s)))
    ba_states = {}
    multi_state = {}
    for ba, states in grouped.items():
        if isinstance(states, list):
            ba_states[ba] = states[0] if states else ""
            if len(states) > 1:
                multi_state[ba] = states
        else:
            ba_states[ba] = states
    return ba_states, multi_state


def select_best_middle_worst(series: pd.Series, k: int = 3):
    series = series.dropna().sort_values()
    if series.empty:
        return {"best": [], "middle": [], "worst": []}
    best = list(series.index[:k])
    worst = list(series.index[-k:]) if len(series) >= k else list(series.index)
    if len(series) <= k:
        middle = list(series.index)
    else:
        mid = len(series) // 2
        start = max(0, mid - k // 2)
        end = min(len(series), start + k)
        start = max(0, end - k)
        middle = list(series.index[start:end])
    return {"best": best, "middle": middle, "worst": worst}


def load_csv(path: Path):
    df = pd.read_csv(path)
    if df.empty:
        raise SystemExit(f"No data in {path}")
    ts_col = df.columns[0]
    counties = list(df.columns[1:])
    # Parse timestamps robustly: allow mixed date-only and datetime strings.
    ts_parsed = pd.to_datetime(df[ts_col], format="ISO8601", errors="coerce")
    if ts_parsed.isna().any():
        ts_parsed = pd.to_datetime(df[ts_col], errors="coerce")
    if ts_parsed.isna().any():
        bad_samples = df.loc[ts_parsed.isna(), ts_col].head(5).tolist()
        raise SystemExit(f"Unparseable timestamps in {path}: {bad_samples}")
    df[ts_col] = ts_parsed
    df = df.set_index(ts_col)
    # If there are duplicate timestamps, keep the last occurrence to match prior dict overwrite behavior.
    df = df[~df.index.duplicated(keep="last")]
    return counties, df[counties]


def align_frames(df_ref: pd.DataFrame, df_reg: pd.DataFrame):
    common_idx = df_ref.index.intersection(df_reg.index).sort_values()
    miss_ref_only = df_ref.index.difference(df_reg.index)
    miss_reg_only = df_reg.index.difference(df_ref.index)
    if common_idx.empty:
        raise SystemExit("No common timestamps")
    df_ref_aligned = df_ref.loc[common_idx]
    df_reg_aligned = df_reg.loc[common_idx]
    return df_ref_aligned, df_reg_aligned, common_idx, miss_ref_only, miss_reg_only


def monthly_totals(obj):
    return obj.groupby(obj.index.month).sum().reindex(range(1, 13), fill_value=0.0)


def seasonal_totals(series: pd.Series):
    return series.groupby(series.index.month.map(SEASON_LOOKUP)).sum().reindex(SEASON_ORDER, fill_value=0.0)


def diurnal_corr_by_month(nat_ref: pd.Series, nat_reg: pd.Series):
    df = pd.DataFrame({"ref": nat_ref, "reg": nat_reg})
    df["month"] = df.index.month
    df["hour"] = df.index.hour
    out = []
    for m in range(1, 13):
        month_slice = df[df["month"] == m]
        if month_slice.empty:
            out.append((m, float("nan")))
            continue
        prof = month_slice.groupby("hour").mean().reindex(range(24), fill_value=np.nan)
        out.append((m, float(prof["ref"].corr(prof["reg"]))))
    return out


def plot_monthly_pct(monthly_pct: pd.Series, out_path: Path):
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.axhline(0, color="k", linewidth=0.8)
    ax.bar(monthly_pct.index, monthly_pct.values, color="#3b82f6")
    ax.set_xticks(range(1, 13))
    ax.set_xlabel("Month")
    ax.set_ylabel("Percent diff (reg - ref) / ref")
    ax.set_title("Monthly percent difference (national)")
    fig.tight_layout()
    fig.savefig(out_path, dpi=200)
    plt.close(fig)


def plot_top_county_pct(county_pct: pd.Series, out_path: Path, label_lookup=None):
    series = pd.concat(
        [
            county_pct.dropna().sort_values(ascending=False).head(5),
            county_pct.dropna().sort_values().head(5),
        ]
    )
    if series.empty:
        return
    if label_lookup is not None:
        series.index = [label_lookup(idx) for idx in series.index]
    colors = ["#16a34a" if v >= 0 else "#dc2626" for v in series.values]
    fig, ax = plt.subplots(figsize=(8, 4))
    series[::-1].plot.barh(ax=ax, color=colors[::-1])
    ax.set_xlabel("Percent diff (reg - ref) / ref")
    ax.set_title("Top +/- county annual percent differences")
    fig.tight_layout()
    fig.savefig(out_path, dpi=200)
    plt.close(fig)


def plot_daily_scatter(daily_ref: pd.Series, daily_reg: pd.Series, out_path: Path):
    fig, ax = plt.subplots(figsize=(5, 5))
    ax.scatter(daily_ref.values, daily_reg.values, alpha=0.6, color="#0ea5e9", edgecolor="white", linewidth=0.5)
    lims = [
        min(daily_ref.min(), daily_reg.min()),
        max(daily_ref.max(), daily_reg.max()),
    ]
    ax.plot(lims, lims, "k--", linewidth=1)
    ax.set_xlim(lims)
    ax.set_ylim(lims)
    ax.set_xlabel("Daily ref total (GWh)")
    ax.set_ylabel("Daily reg total (GWh)")
    ax.set_title("Daily totals: reg vs ref")
    fig.tight_layout()
    fig.savefig(out_path, dpi=200)
    plt.close(fig)


def plot_hourly_scatter(nat_ref: pd.Series, nat_reg: pd.Series, out_path: Path, title: str = None):
    aligned = pd.concat([nat_ref, nat_reg], axis=1, join="inner").dropna()
    aligned.columns = ["ref", "reg"]
    if aligned.empty:
        return
    fig, ax = plt.subplots(figsize=(5, 5))
    ax.scatter(aligned["ref"].values, aligned["reg"].values, alpha=0.3, color="#6366f1", edgecolor="white", linewidth=0.3, s=8)
    lims = [
        min(aligned["ref"].min(), aligned["reg"].min()),
        max(aligned["ref"].max(), aligned["reg"].max()),
    ]
    ax.plot(lims, lims, "k--", linewidth=1)
    ax.set_xlim(lims)
    ax.set_ylim(lims)
    ax.set_xlabel("Hourly ref total (GWh)")
    ax.set_ylabel("Hourly reg total (GWh)")
    ax.set_title(title or "Hourly totals: reg vs ref")
    fig.tight_layout()
    fig.savefig(out_path, dpi=200)
    plt.close(fig)


def plot_ranked_hourly_scatters(df_ref: pd.DataFrame, df_reg: pd.DataFrame, ranking: pd.Series, label_lookup, prefix: str):
    targets = select_best_middle_worst(ranking)
    for bucket, ids in targets.items():
        for idx in ids:
            if idx not in df_ref.columns or idx not in df_reg.columns:
                continue
            title = f"{label_lookup(idx)} ({bucket})"
            out_path = OUTPUT_DIR / f"{prefix}_{bucket}_{idx}.png"
            plot_hourly_scatter(df_ref[idx], df_reg[idx], out_path, title=title)


def main():
    OUTPUT_DIR.mkdir(exist_ok=True)

    counties_ref, df_ref_raw = load_csv(REF_PATH)
    counties_reg, df_reg_raw = load_csv(REG_PATH)
    if counties_ref != counties_reg:
        raise SystemExit("Header mismatch")
    rename_map = {c: normalize_fips(c) for c in counties_ref}
    df_ref_raw = df_ref_raw.rename(columns=rename_map)
    df_reg_raw = df_reg_raw.rename(columns=rename_map)
    counties = [rename_map[c] for c in counties_ref]
    n = len(counties)

    df_ref, df_reg, common_idx, miss_ref_only, miss_reg_only = align_frames(df_ref_raw, df_reg_raw)

    county_meta, missing_meta = load_county_metadata(counties)
    if missing_meta:
        missing_preview = ", ".join(missing_meta[:5])
        print(f"Counties missing in mapping file: {len(missing_meta)} ({missing_preview}{'...' if len(missing_meta) > 5 else ''})")
    ba_states, multi_state = ba_state_lookup_from_meta(county_meta)
    if multi_state:
        example_ba = next(iter(multi_state))
        print(f"Warning: BA(s) with multiple states in mapping (example {example_ba}: {multi_state[example_ba]})")
    fmt_county = lambda f: county_label(f, county_meta)
    fmt_ba = lambda b: ba_label(b, ba_states)

    nat_ref = df_ref.sum(axis=1)
    nat_reg = df_reg.sum(axis=1)

    monthly_nat_ref = monthly_totals(nat_ref)
    monthly_nat_reg = monthly_totals(nat_reg)
    season_ref = seasonal_totals(nat_ref)
    season_reg = seasonal_totals(nat_reg)

    county_ann_ref = df_ref.sum(axis=0)
    county_ann_reg = df_reg.sum(axis=0)
    monthly_county_ref = monthly_totals(df_ref)
    monthly_county_reg = monthly_totals(df_reg)
    if not county_meta.empty:
        df_ref_ba = df_ref[county_meta.index].groupby(county_meta["ba"], axis=1).sum()
        df_reg_ba = df_reg[county_meta.index].groupby(county_meta["ba"], axis=1).sum()
    else:
        df_ref_ba = pd.DataFrame(index=df_ref.index)
        df_reg_ba = pd.DataFrame(index=df_reg.index)

    annual_ref = float(nat_ref.sum())
    annual_reg = float(nat_reg.sum())
    annual_pct = (annual_reg - annual_ref) / annual_ref * 100 if annual_ref else float("nan")

    monthly_pct = (monthly_nat_reg - monthly_nat_ref).div(monthly_nat_ref.replace(0, np.nan)) * 100
    seasonal_pct = (season_reg - season_ref).div(season_ref.replace(0, np.nan)) * 100

    county_pct = (county_ann_reg - county_ann_ref).div(county_ann_ref.replace(0, np.nan)) * 100
    abs_pct = county_pct.dropna().abs()
    mean_abs = float(abs_pct.mean()) if not abs_pct.empty else float("nan")
    median_abs = float(abs_pct.median()) if not abs_pct.empty else float("nan")
    max_pos = county_pct.idxmax() if not county_pct.dropna().empty else None
    max_neg = county_pct.idxmin() if not county_pct.dropna().empty else None

    pos5 = county_pct.dropna().sort_values(ascending=False).head(5)
    neg5 = county_pct.dropna().sort_values().head(5)

    if not df_ref_ba.empty:
        ba_ann_ref = df_ref_ba.sum(axis=0)
        ba_ann_reg = df_reg_ba.sum(axis=0)
        ba_pct = (ba_ann_reg - ba_ann_ref).div(ba_ann_ref.replace(0, np.nan)) * 100
        ba_abs_pct = ba_pct.dropna().abs()
        ba_mean_abs = float(ba_abs_pct.mean()) if not ba_abs_pct.empty else float("nan")
        ba_median_abs = float(ba_abs_pct.median()) if not ba_abs_pct.empty else float("nan")
        ba_max_pos = ba_pct.idxmax() if not ba_pct.dropna().empty else None
        ba_max_neg = ba_pct.idxmin() if not ba_pct.dropna().empty else None
        ba_pos5 = ba_pct.dropna().sort_values(ascending=False).head(5)
        ba_neg5 = ba_pct.dropna().sort_values().head(5)
    else:
        ba_ann_ref = pd.Series(dtype=float)
        ba_ann_reg = pd.Series(dtype=float)
        ba_pct = pd.Series(dtype=float)
        ba_mean_abs = float("nan")
        ba_median_abs = float("nan")
        ba_max_pos = None
        ba_max_neg = None
        ba_pos5 = pd.Series(dtype=float)
        ba_neg5 = pd.Series(dtype=float)

    mape_cm = (monthly_county_reg - monthly_county_ref).abs().div(monthly_county_ref.replace(0, np.nan))
    mape_cm_flat = mape_cm.stack().dropna()
    mean_mape_cm = float(mape_cm_flat.mean() * 100) if not mape_cm_flat.empty else float("nan")
    median_mape_cm = float(mape_cm_flat.median() * 100) if not mape_cm_flat.empty else float("nan")
    if not df_ref_ba.empty:
        monthly_ba_ref = monthly_totals(df_ref_ba)
        monthly_ba_reg = monthly_totals(df_reg_ba)
        mape_ba = (monthly_ba_reg - monthly_ba_ref).abs().div(monthly_ba_ref.replace(0, np.nan))
        mape_ba_flat = mape_ba.stack().dropna()
        mean_mape_ba = float(mape_ba_flat.mean() * 100) if not mape_ba_flat.empty else float("nan")
        median_mape_ba = float(mape_ba_flat.median() * 100) if not mape_ba_flat.empty else float("nan")
    else:
        mean_mape_ba = float("nan")
        median_mape_ba = float("nan")

    hourly_corr = float(nat_ref.corr(nat_reg))

    daily_ref = nat_ref.resample("D").sum()
    daily_reg = nat_reg.resample("D").sum()
    daily_corr = float(daily_ref.corr(daily_reg))

    monthly_diurnal_corr = diurnal_corr_by_month(nat_ref, nat_reg)

    peak_ref_val = float(nat_ref.max())
    peak_reg_val = float(nat_reg.max())
    peak_ref_ts = nat_ref.idxmax()
    peak_reg_ts = nat_reg.idxmax()
    peak_diff_pct = (peak_reg_val - peak_ref_val) / peak_ref_val * 100 if peak_ref_val else float("nan")
    reg_at_ref_peak = float(nat_reg.loc[peak_ref_ts])
    ref_at_reg_peak = float(nat_ref.loc[peak_reg_ts])

    ref_pct = nat_ref.quantile([p / 100 for p in PCTL])
    reg_pct = nat_reg.quantile([p / 100 for p in PCTL])

    lf_ref = annual_ref / (peak_ref_val * len(nat_ref)) if peak_ref_val else float("nan")
    lf_reg = annual_reg / (peak_reg_val * len(nat_ref)) if peak_reg_val else float("nan")

    nat_mae = float(np.abs(nat_reg - nat_ref).mean())
    nat_nmae = nat_mae / nat_ref.mean() if nat_ref.mean() else float("nan")
    nat_rmse = float(np.sqrt(((nat_reg - nat_ref) ** 2).mean()))

    county_mae = (df_reg - df_ref).abs().mean()
    county_nmae = county_mae.div(df_ref.mean().replace(0, np.nan))
    county_rmse = np.sqrt(((df_reg - df_ref) ** 2).mean())
    mean_rmse = float(county_rmse.mean())
    median_rmse = float(county_rmse.median())
    max_rmse_idx = county_rmse.idxmax()
    min_rmse_idx = county_rmse.idxmin()

    mean_mae = float(county_mae.mean())
    median_mae = float(county_mae.median())
    mean_nmae = float(county_nmae.mean())
    median_nmae = float(county_nmae.median())
    if county_nmae.dropna().empty:
        min_nmae_idx = None
        max_nmae_idx = None
    else:
        min_nmae_idx = county_nmae.idxmin()
        max_nmae_idx = county_nmae.idxmax()

    if not df_ref_ba.empty:
        ba_mae = (df_reg_ba - df_ref_ba).abs().mean()
        ba_nmae = ba_mae.div(df_ref_ba.mean().replace(0, np.nan))
        ba_rmse = np.sqrt(((df_reg_ba - df_ref_ba) ** 2).mean())
        mean_ba_mae = float(ba_mae.mean())
        median_ba_mae = float(ba_mae.median())
        mean_ba_nmae = float(ba_nmae.mean())
        median_ba_nmae = float(ba_nmae.median())
        mean_ba_rmse = float(ba_rmse.mean())
        median_ba_rmse = float(ba_rmse.median())
        if ba_nmae.dropna().empty:
            min_ba_nmae_idx = None
            max_ba_nmae_idx = None
        else:
            min_ba_nmae_idx = ba_nmae.idxmin()
            max_ba_nmae_idx = ba_nmae.idxmax()
        if ba_rmse.dropna().empty:
            ba_max_rmse_idx = None
            ba_min_rmse_idx = None
        else:
            ba_max_rmse_idx = ba_rmse.idxmax()
            ba_min_rmse_idx = ba_rmse.idxmin()
    else:
        ba_mae = pd.Series(dtype=float)
        ba_nmae = pd.Series(dtype=float)
        ba_rmse = pd.Series(dtype=float)
        mean_ba_mae = float("nan")
        median_ba_mae = float("nan")
        mean_ba_nmae = float("nan")
        median_ba_nmae = float("nan")
        mean_ba_rmse = float("nan")
        median_ba_rmse = float("nan")
        min_ba_nmae_idx = None
        max_ba_nmae_idx = None
        ba_max_rmse_idx = None
        ba_min_rmse_idx = None

    county_peak_ref = df_ref.max()
    county_peak_reg = df_reg.max()
    county_peak_pct = (county_peak_reg - county_peak_ref).div(county_peak_ref.replace(0, np.nan)) * 100
    finite_peak = county_peak_pct.dropna()
    peak_mean = float(finite_peak.mean()) if not finite_peak.empty else float("nan")
    peak_median = float(finite_peak.median()) if not finite_peak.empty else float("nan")
    if finite_peak.empty:
        peak_max_idx = None
        peak_min_idx = None
    else:
        peak_max_idx = county_peak_pct.idxmax()
        peak_min_idx = county_peak_pct.idxmin()

    print("Ref rows:", len(df_ref_raw), "Reg rows:", len(df_reg_raw), "Common:", len(common_idx))
    print("Missing in reg:", len(miss_ref_only), "Missing in ref:", len(miss_reg_only))

    print("Hours used:", len(common_idx), "Counties:", n)
    print("Annual GWh ref={:,.2f}, reg={:,.2f}, pct diff={:.2f}%".format(annual_ref, annual_reg, annual_pct))
    print("\nSeasonal pct diff (reg-ref)/ref:")
    for k in SEASON_ORDER:
        print(f"  {k}: {seasonal_pct[k]:.2f}% (ref {season_ref[k]:,.2f} GWh, reg {season_reg[k]:,.2f} GWh)")

    print("\nMonthly pct diff (reg-ref)/ref:")
    for m in range(1, 13):
        print(f"  {m:02d}: {monthly_pct[m]:.2f}%")

    print("\nCounty annual pct diff stats:")
    print(f"  Mean abs pct diff: {mean_abs:.2f}%")
    print(f"  Median abs pct diff: {median_abs:.2f}%")
    if max_pos is not None and max_neg is not None:
        print(f"  Max positive pct diff: {county_pct[max_pos]:.2f}% ({fmt_county(max_pos)})")
        print(f"  Max negative pct diff: {county_pct[max_neg]:.2f}% ({fmt_county(max_neg)})")
    print("  Top +5 counties (pct diff):")
    for fips, pct in pos5.items():
        print(f"    {fmt_county(fips)}: {pct:.2f}%")
    print("  Top -5 counties (pct diff):")
    for fips, pct in neg5.items():
        print(f"    {fmt_county(fips)}: {pct:.2f}%")

    if not ba_pct.empty:
        print("\nBA annual pct diff stats:")
        print(f"  Mean abs pct diff: {ba_mean_abs:.2f}%")
        print(f"  Median abs pct diff: {ba_median_abs:.2f}%")
        if ba_max_pos is not None and ba_max_neg is not None:
            print(f"  Max positive pct diff: {ba_pct[ba_max_pos]:.2f}% ({fmt_ba(ba_max_pos)})")
            print(f"  Max negative pct diff: {ba_pct[ba_max_neg]:.2f}% ({fmt_ba(ba_max_neg)})")
        print("  Top +5 BAs (pct diff):")
        for ba, pct in ba_pos5.items():
            print(f"    {fmt_ba(ba)}: {pct:.2f}%")
        print("  Top -5 BAs (pct diff):")
        for ba, pct in ba_neg5.items():
            print(f"    {fmt_ba(ba)}: {pct:.2f}%")

    print("\nCounty-month MAPE (ref>0): mean {:.2f}%, median {:.2f}%".format(mean_mape_cm, median_mape_cm))
    if not df_ref_ba.empty:
        print("BA-month MAPE (ref>0): mean {:.2f}%, median {:.2f}%".format(mean_mape_ba, median_mape_ba))
    print("National hourly correlation: {:.4f}".format(hourly_corr))
    print("Daily-total correlation: {:.4f}".format(daily_corr))
    print("Load factor ref={:.3f}, reg={:.3f}, delta={:.3f}".format(lf_ref, lf_reg, lf_reg - lf_ref))

    print("\nDiurnal profile correlation by month (0-23 avg hour):")
    for m, c in monthly_diurnal_corr:
        print(f"  Month {m:02d}: {c:.4f}")

    nat_nmae_pct = nat_nmae * 100 if np.isfinite(nat_nmae) else float("nan")
    print("\nNational hourly error metrics:")
    print("  MAE: {:.3f} GWh".format(nat_mae))
    print("  Normalized MAE (MAE/mean ref): {:.4f} ({:.2f}%)".format(nat_nmae, nat_nmae_pct))
    print("  RMSE: {:.3f} GWh".format(nat_rmse))

    print("\nCounty hourly error stats:")
    print("  MAE (GWh): mean {:.3f}, median {:.3f}".format(mean_mae, median_mae))
    if min_nmae_idx is not None and max_nmae_idx is not None:
        print(
            "  Normalized MAE: mean {:.4f}, median {:.4f}, best {:.4f} ({}), worst {:.4f} ({})".format(
                mean_nmae,
                median_nmae,
                county_nmae[min_nmae_idx],
                fmt_county(min_nmae_idx),
                county_nmae[max_nmae_idx],
                fmt_county(max_nmae_idx),
            )
        )
    else:
        print("  Normalized MAE: mean {:.4f}, median {:.4f}".format(mean_nmae, median_nmae))
    print(
        "  RMSE (GWh): mean {:.3f}, median {:.3f}, max {:.3f} ({}), min {:.3f} ({})".format(
            mean_rmse,
            median_rmse,
            county_rmse[max_rmse_idx],
            fmt_county(max_rmse_idx),
            county_rmse[min_rmse_idx],
            fmt_county(min_rmse_idx),
        )
    )

    if not ba_rmse.empty:
        print("\nBA hourly error stats:")
        print("  MAE (GWh): mean {:.3f}, median {:.3f}".format(mean_ba_mae, median_ba_mae))
        if min_ba_nmae_idx is not None and max_ba_nmae_idx is not None:
            print(
                "  Normalized MAE: mean {:.4f}, median {:.4f}, best {:.4f} ({}), worst {:.4f} ({})".format(
                    mean_ba_nmae,
                    median_ba_nmae,
                    ba_nmae[min_ba_nmae_idx],
                    fmt_ba(min_ba_nmae_idx),
                    ba_nmae[max_ba_nmae_idx],
                    fmt_ba(max_ba_nmae_idx),
                )
            )
        else:
            print("  Normalized MAE: mean {:.4f}, median {:.4f}".format(mean_ba_nmae, median_ba_nmae))
        if ba_max_rmse_idx is not None and ba_min_rmse_idx is not None:
            print(
                "  RMSE (GWh): mean {:.3f}, median {:.3f}, max {:.3f} ({}), min {:.3f} ({})".format(
                    mean_ba_rmse,
                    median_ba_rmse,
                    ba_rmse[ba_max_rmse_idx],
                    fmt_ba(ba_max_rmse_idx),
                    ba_rmse[ba_min_rmse_idx],
                    fmt_ba(ba_min_rmse_idx),
                )
            )
        else:
            print("  RMSE (GWh): mean {:.3f}, median {:.3f}".format(mean_ba_rmse, median_ba_rmse))

    print("\nPeak demand (national totals, GWh):")
    print("  Ref peak: {:.3f} at {}".format(peak_ref_val, peak_ref_ts))
    print("  Reg peak: {:.3f} at {}".format(peak_reg_val, peak_reg_ts))
    print("  Peak percent diff: {:.2f}%".format(peak_diff_pct))
    print(
        "  Reg at ref-peak hour: {:.3f} (pct diff vs ref {:.2f}%)".format(
            reg_at_ref_peak, (reg_at_ref_peak - peak_ref_val) / peak_ref_val * 100 if peak_ref_val else float("nan")
        )
    )
    print(
        "  Ref at reg-peak hour: {:.3f} (pct diff vs reg {:.2f}%)".format(
            ref_at_reg_peak, (ref_at_reg_peak - peak_reg_val) / peak_reg_val * 100 if peak_reg_val else float("nan")
        )
    )

    print("\nNational high-load thresholds (GWh):")
    for p in PCTL:
        pdiff = (reg_pct.loc[p / 100] - ref_pct.loc[p / 100]) / ref_pct.loc[p / 100] * 100
        print("  {:>3}%: ref {:.3f}, reg {:.3f}, pct diff {:.2f}%".format(p, ref_pct.loc[p / 100], reg_pct.loc[p / 100], pdiff))

    print(
        "\nCounty peak percent diff (reg-ref)/ref: mean {:.2f}%, median {:.2f}%".format(
            peak_mean, peak_median
        )
    )
    if peak_max_idx is not None and peak_min_idx is not None:
        print(
            "  Max {:.2f}% ({}), Min {:.2f}% ({})".format(
                county_peak_pct[peak_max_idx],
                fmt_county(peak_max_idx),
                county_peak_pct[peak_min_idx],
                fmt_county(peak_min_idx),
            )
        )

    plot_monthly_pct(monthly_pct, OUTPUT_DIR / "monthly_percent_diff.png")
    plot_top_county_pct(county_pct, OUTPUT_DIR / "county_percent_diff_top.png", label_lookup=fmt_county)
    plot_daily_scatter(daily_ref, daily_reg, OUTPUT_DIR / "daily_totals_scatter.png")
    plot_hourly_scatter(nat_ref, nat_reg, OUTPUT_DIR / "hourly_totals_scatter.png")
    plot_ranked_hourly_scatters(df_ref, df_reg, county_nmae, fmt_county, "county_hourly_scatter")
    if not ba_nmae.empty:
        plot_ranked_hourly_scatters(df_ref_ba, df_reg_ba, ba_nmae, fmt_ba, "ba_hourly_scatter")
    print(f"\nCharts written to {OUTPUT_DIR.resolve()}")


if __name__ == "__main__":
    main()
