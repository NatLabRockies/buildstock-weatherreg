'''
This file is for validating regressed electricity demand data from resstock and comstock. The "ref" data is aggregated electricity demand directly from resstock/comstock for a given weather year, while the "reg" data is regressed data for the same weather year, generated by geo_predict, which is trained on resstock/comstock data (and associated weather data) from a different weather year.

This script was tested with the reeds2 conda environment.
'''

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from pathlib import Path
import shutil

#Input the regression and reference file paths here.
REF_PATH = Path(r"C:\ReEDS\geo_predict\validation_2025-12-19\com_0_ref_outputs_2025-12-10-15-53-30\agg_com_eulp_hvac_elec_GWh_upgrade0.csv")
REG_PATH = Path(r"C:\ReEDS\geo_predict\validation_2025-12-19\com_0_reg_outputs_2025-12-02-09-29-48\agg_com_eulp_hvac_elec_GWh_upgrade0.csv")
OUTPUT_DIR = Path("validation_outputs")
COUNTY_MAP_URL = "https://raw.githubusercontent.com/NREL/ReEDS-2.0/refs/heads/main/inputs/county2zone.csv"

SEASON_LOOKUP = {
    12: "DJF",
    1: "DJF",
    2: "DJF",
    3: "MAM",
    4: "MAM",
    5: "MAM",
    6: "JJA",
    7: "JJA",
    8: "JJA",
    9: "SON",
    10: "SON",
    11: "SON",
}
SEASON_ORDER = ["DJF", "MAM", "JJA", "SON"]
PCTL = [90, 95, 99]


def escape_html(val):
    text = str(val)
    return (
        text.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
        .replace("'", "&#39;")
    )


def html_table(rows, headers):
    head_cells = "".join(f"<th>{escape_html(h)}</th>" for h in headers)
    body_rows = []
    for row in rows:
        cells = "".join(f"<td>{escape_html(val)}</td>" for val in row)
        body_rows.append(f"<tr>{cells}</tr>")
    return f"<table><thead><tr>{head_cells}</tr></thead><tbody>{''.join(body_rows)}</tbody></table>"


def bullet_list(items):
    return "<ul>" + "".join(f"<li>{escape_html(item)}</li>" for item in items) + "</ul>"


def fmt_pct(val):
    return "nan" if pd.isna(val) else f"{val:.2f}%"


def fmt_gwh(val):
    return "nan" if pd.isna(val) else f"{val:,.3f} GWh"


def fmt_float(val, precision=4):
    return "nan" if pd.isna(val) else f"{val:.{precision}f}"


def ensure_output_dir(path: Path):
    if path.exists():
        contents = list(path.iterdir())
        if contents:
            resp = input(f"Output directory '{path}' exists and has {len(contents)} item(s). Delete it? [y/N]: ").strip().lower()
            if resp not in ("y", "yes"):
                raise SystemExit("Aborting; output directory not cleared.")
            shutil.rmtree(path)
    path.mkdir(parents=True, exist_ok=True)


def normalize_fips(val) -> str:
    sval = str(val).strip()
    sval = sval.lstrip("0")
    return sval or "0"


def load_county_metadata(counties):
    try:
        mapping = pd.read_csv(COUNTY_MAP_URL, dtype={"FIPS": str})
    except Exception as exc:
        raise SystemExit(f"Failed to load county mapping from {COUNTY_MAP_URL}: {exc}")
    mapping["FIPS"] = mapping["FIPS"].astype(str).str.strip()
    mapping["ba"] = mapping["ba"].astype(str).str.strip()
    mapping["fips_norm"] = mapping["FIPS"].apply(normalize_fips)
    mapping["county_name"] = mapping["county_name"].astype(str).str.strip()
    mapping["state"] = mapping["state"].astype(str).str.strip()
    subset = mapping[mapping["fips_norm"].isin(counties)].drop_duplicates(subset=["fips_norm"]).set_index("fips_norm")
    missing = sorted(set(counties) - set(subset.index))
    return subset, missing


def county_label(fips: str, county_meta: pd.DataFrame):
    if fips in county_meta.index:
        row = county_meta.loc[fips]
        name = row["county_name"].title()
        state = row["state"]
        return f"{name} County, {state} ({fips})"
    return f"FIPS {fips}"


def ba_label(ba: str, ba_states: dict):
    state = ba_states.get(ba)
    return f"BA {ba} ({state})" if state else f"BA {ba}"


def ba_state_lookup_from_meta(county_meta: pd.DataFrame):
    grouped = county_meta.groupby("ba")["state"].agg(lambda s: sorted(set(s)))
    ba_states = {}
    multi_state = {}
    for ba, states in grouped.items():
        if isinstance(states, list):
            ba_states[ba] = states[0] if states else ""
            if len(states) > 1:
                multi_state[ba] = states
        else:
            ba_states[ba] = states
    return ba_states, multi_state


def select_best_middle_worst(series: pd.Series, k: int = 3):
    series = series.dropna().sort_values()
    if series.empty:
        return {"best": [], "middle": [], "worst": []}
    best = list(series.index[:k])
    worst = list(series.index[-k:]) if len(series) >= k else list(series.index)
    if len(series) <= k:
        middle = list(series.index)
    else:
        mid = len(series) // 2
        start = max(0, mid - k // 2)
        end = min(len(series), start + k)
        start = max(0, end - k)
        middle = list(series.index[start:end])
    return {"best": best, "middle": middle, "worst": worst}


def load_csv(path: Path):
    df = pd.read_csv(path)
    if df.empty:
        raise SystemExit(f"No data in {path}")
    ts_col = df.columns[0]
    counties = list(df.columns[1:])
    # Parse timestamps robustly: allow mixed date-only and datetime strings.
    ts_parsed = pd.to_datetime(df[ts_col], format="ISO8601", errors="coerce")
    if ts_parsed.isna().any():
        ts_parsed = pd.to_datetime(df[ts_col], errors="coerce")
    if ts_parsed.isna().any():
        bad_samples = df.loc[ts_parsed.isna(), ts_col].head(5).tolist()
        raise SystemExit(f"Unparseable timestamps in {path}: {bad_samples}")
    df[ts_col] = ts_parsed
    df = df.set_index(ts_col)
    # If there are duplicate timestamps, keep the last occurrence to match prior dict overwrite behavior.
    df = df[~df.index.duplicated(keep="last")]
    return counties, df[counties]


def align_frames(df_ref: pd.DataFrame, df_reg: pd.DataFrame):
    common_idx = df_ref.index.intersection(df_reg.index).sort_values()
    miss_ref_only = df_ref.index.difference(df_reg.index)
    miss_reg_only = df_reg.index.difference(df_ref.index)
    if common_idx.empty:
        raise SystemExit("No common timestamps")
    df_ref_aligned = df_ref.loc[common_idx]
    df_reg_aligned = df_reg.loc[common_idx]
    return df_ref_aligned, df_reg_aligned, common_idx, miss_ref_only, miss_reg_only


def monthly_totals(obj):
    return obj.groupby(obj.index.month).sum().reindex(range(1, 13), fill_value=0.0)


def seasonal_totals(series: pd.Series):
    return series.groupby(series.index.month.map(SEASON_LOOKUP)).sum().reindex(SEASON_ORDER, fill_value=0.0)


def diurnal_corr_by_month(nat_ref: pd.Series, nat_reg: pd.Series):
    df = pd.DataFrame({"ref": nat_ref, "reg": nat_reg})
    df["month"] = df.index.month
    df["hour"] = df.index.hour
    out = []
    for m in range(1, 13):
        month_slice = df[df["month"] == m]
        if month_slice.empty:
            out.append((m, float("nan")))
            continue
        prof = month_slice.groupby("hour").mean().reindex(range(24), fill_value=np.nan)
        out.append((m, float(prof["ref"].corr(prof["reg"]))))
    return out


def plot_monthly_pct(monthly_pct: pd.Series, out_path: Path):
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.axhline(0, color="k", linewidth=0.8)
    ax.bar(monthly_pct.index, monthly_pct.values, color="#3b82f6")
    ax.set_xticks(range(1, 13))
    ax.set_xlabel("Month")
    ax.set_ylabel("Percent diff (reg - ref) / ref")
    ax.set_title("Monthly percent difference (national)")
    fig.tight_layout()
    fig.savefig(out_path, dpi=200)
    plt.close(fig)


def plot_top_county_pct(county_pct: pd.Series, out_path: Path, label_lookup=None):
    series = pd.concat(
        [
            county_pct.dropna().sort_values(ascending=False).head(5),
            county_pct.dropna().sort_values().head(5),
        ]
    )
    if series.empty:
        return
    if label_lookup is not None:
        series.index = [label_lookup(idx) for idx in series.index]
    colors = ["#16a34a" if v >= 0 else "#dc2626" for v in series.values]
    fig, ax = plt.subplots(figsize=(8, 4))
    series[::-1].plot.barh(ax=ax, color=colors[::-1])
    ax.set_xlabel("Percent diff (reg - ref) / ref")
    ax.set_title("Top +/- county annual percent differences")
    fig.tight_layout()
    fig.savefig(out_path, dpi=200)
    plt.close(fig)


def plot_daily_scatter(daily_ref: pd.Series, daily_reg: pd.Series, out_path: Path):
    fig, ax = plt.subplots(figsize=(5, 5))
    ax.scatter(daily_ref.values, daily_reg.values, alpha=0.6, color="#0ea5e9", edgecolor="white", linewidth=0.5)
    lims = [
        min(daily_ref.min(), daily_reg.min()),
        max(daily_ref.max(), daily_reg.max()),
    ]
    ax.plot(lims, lims, "k--", linewidth=1)
    ax.set_xlim(lims)
    ax.set_ylim(lims)
    ax.set_xlabel("Daily ref total (GWh)")
    ax.set_ylabel("Daily reg total (GWh)")
    ax.set_title("Daily totals: reg vs ref")
    fig.tight_layout()
    fig.savefig(out_path, dpi=200)
    plt.close(fig)


def plot_hourly_scatter(nat_ref: pd.Series, nat_reg: pd.Series, out_path: Path, title: str = None):
    aligned = pd.concat([nat_ref, nat_reg], axis=1, join="inner").dropna()
    aligned.columns = ["ref", "reg"]
    if aligned.empty:
        return
    diff = (aligned["reg"] - aligned["ref"]).abs()
    mae = float(diff.mean())
    ref_mean = float(aligned["ref"].mean())
    nmae = mae / ref_mean if ref_mean else float("nan")
    fig, ax = plt.subplots(figsize=(5, 5))
    ax.scatter(aligned["ref"].values, aligned["reg"].values, alpha=0.3, color="#6366f1", edgecolor="white", linewidth=0.3, s=8)
    lims = [
        min(aligned["ref"].min(), aligned["reg"].min()),
        max(aligned["ref"].max(), aligned["reg"].max()),
    ]
    ax.plot(lims, lims, "k--", linewidth=1)
    ax.set_xlim(lims)
    ax.set_ylim(lims)
    ax.set_xlabel("Hourly ref total (GWh)")
    ax.set_ylabel("Hourly reg total (GWh)")
    ax.set_title(title or "Hourly totals: reg vs ref")
    mae_txt = f"MAE: {mae:.3f} GWh"
    if np.isfinite(nmae):
        mae_txt += f"\nNMAE: {nmae*100:.2f}%"
    ax.text(
        0.05,
        0.95,
        mae_txt,
        transform=ax.transAxes,
        va="top",
        ha="left",
        fontsize=8.5,
        bbox=dict(facecolor="white", alpha=0.8, edgecolor="none"),
    )
    fig.tight_layout()
    fig.savefig(out_path, dpi=200)
    plt.close(fig)


def plot_ranked_hourly_scatters(df_ref: pd.DataFrame, df_reg: pd.DataFrame, ranking: pd.Series, label_lookup, prefix: str):
    targets = select_best_middle_worst(ranking)
    for bucket, ids in targets.items():
        for idx in ids:
            if idx not in df_ref.columns or idx not in df_reg.columns:
                continue
            title = f"{label_lookup(idx)} ({bucket})"
            out_path = OUTPUT_DIR / f"{prefix}_{bucket}_{idx}.png"
            plot_hourly_scatter(df_ref[idx], df_reg[idx], out_path, title=title)


def main():
    ensure_output_dir(OUTPUT_DIR)

    counties_ref, df_ref_raw = load_csv(REF_PATH)
    counties_reg, df_reg_raw = load_csv(REG_PATH)
    if counties_ref != counties_reg:
        raise SystemExit("Header mismatch")
    rename_map = {c: normalize_fips(c) for c in counties_ref}
    df_ref_raw = df_ref_raw.rename(columns=rename_map)
    df_reg_raw = df_reg_raw.rename(columns=rename_map)
    counties = [rename_map[c] for c in counties_ref]
    n = len(counties)

    df_ref, df_reg, common_idx, miss_ref_only, miss_reg_only = align_frames(df_ref_raw, df_reg_raw)

    county_meta, missing_meta = load_county_metadata(counties)
    ba_states, multi_state = ba_state_lookup_from_meta(county_meta)
    fmt_county = lambda f: county_label(f, county_meta)
    fmt_ba = lambda b: ba_label(b, ba_states)

    nat_ref = df_ref.sum(axis=1)
    nat_reg = df_reg.sum(axis=1)

    monthly_nat_ref = monthly_totals(nat_ref)
    monthly_nat_reg = monthly_totals(nat_reg)
    season_ref = seasonal_totals(nat_ref)
    season_reg = seasonal_totals(nat_reg)

    county_ann_ref = df_ref.sum(axis=0)
    county_ann_reg = df_reg.sum(axis=0)
    monthly_county_ref = monthly_totals(df_ref)
    monthly_county_reg = monthly_totals(df_reg)
    if not county_meta.empty:
        df_ref_ba = df_ref[county_meta.index].groupby(county_meta["ba"], axis=1).sum()
        df_reg_ba = df_reg[county_meta.index].groupby(county_meta["ba"], axis=1).sum()
    else:
        df_ref_ba = pd.DataFrame(index=df_ref.index)
        df_reg_ba = pd.DataFrame(index=df_reg.index)

    annual_ref = float(nat_ref.sum())
    annual_reg = float(nat_reg.sum())
    annual_pct = (annual_reg - annual_ref) / annual_ref * 100 if annual_ref else float("nan")

    monthly_pct = (monthly_nat_reg - monthly_nat_ref).div(monthly_nat_ref.replace(0, np.nan)) * 100
    seasonal_pct = (season_reg - season_ref).div(season_ref.replace(0, np.nan)) * 100

    county_pct = (county_ann_reg - county_ann_ref).div(county_ann_ref.replace(0, np.nan)) * 100
    abs_pct = county_pct.dropna().abs()
    mean_abs = float(abs_pct.mean()) if not abs_pct.empty else float("nan")
    median_abs = float(abs_pct.median()) if not abs_pct.empty else float("nan")
    max_pos = county_pct.idxmax() if not county_pct.dropna().empty else None
    max_neg = county_pct.idxmin() if not county_pct.dropna().empty else None

    pos5 = county_pct.dropna().sort_values(ascending=False).head(5)
    neg5 = county_pct.dropna().sort_values().head(5)

    if not df_ref_ba.empty:
        ba_ann_ref = df_ref_ba.sum(axis=0)
        ba_ann_reg = df_reg_ba.sum(axis=0)
        ba_pct = (ba_ann_reg - ba_ann_ref).div(ba_ann_ref.replace(0, np.nan)) * 100
        ba_abs_pct = ba_pct.dropna().abs()
        ba_mean_abs = float(ba_abs_pct.mean()) if not ba_abs_pct.empty else float("nan")
        ba_median_abs = float(ba_abs_pct.median()) if not ba_abs_pct.empty else float("nan")
        ba_max_pos = ba_pct.idxmax() if not ba_pct.dropna().empty else None
        ba_max_neg = ba_pct.idxmin() if not ba_pct.dropna().empty else None
        ba_pos5 = ba_pct.dropna().sort_values(ascending=False).head(5)
        ba_neg5 = ba_pct.dropna().sort_values().head(5)
    else:
        ba_ann_ref = pd.Series(dtype=float)
        ba_ann_reg = pd.Series(dtype=float)
        ba_pct = pd.Series(dtype=float)
        ba_mean_abs = float("nan")
        ba_median_abs = float("nan")
        ba_max_pos = None
        ba_max_neg = None
        ba_pos5 = pd.Series(dtype=float)
        ba_neg5 = pd.Series(dtype=float)

    mape_cm = (monthly_county_reg - monthly_county_ref).abs().div(monthly_county_ref.replace(0, np.nan))
    mape_cm_flat = mape_cm.stack().dropna()
    mean_mape_cm = float(mape_cm_flat.mean() * 100) if not mape_cm_flat.empty else float("nan")
    median_mape_cm = float(mape_cm_flat.median() * 100) if not mape_cm_flat.empty else float("nan")
    if not df_ref_ba.empty:
        monthly_ba_ref = monthly_totals(df_ref_ba)
        monthly_ba_reg = monthly_totals(df_reg_ba)
        mape_ba = (monthly_ba_reg - monthly_ba_ref).abs().div(monthly_ba_ref.replace(0, np.nan))
        mape_ba_flat = mape_ba.stack().dropna()
        mean_mape_ba = float(mape_ba_flat.mean() * 100) if not mape_ba_flat.empty else float("nan")
        median_mape_ba = float(mape_ba_flat.median() * 100) if not mape_ba_flat.empty else float("nan")
    else:
        mean_mape_ba = float("nan")
        median_mape_ba = float("nan")

    hourly_corr = float(nat_ref.corr(nat_reg))

    daily_ref = nat_ref.resample("D").sum()
    daily_reg = nat_reg.resample("D").sum()
    daily_corr = float(daily_ref.corr(daily_reg))

    monthly_diurnal_corr = diurnal_corr_by_month(nat_ref, nat_reg)

    peak_ref_val = float(nat_ref.max())
    peak_reg_val = float(nat_reg.max())
    peak_ref_ts = nat_ref.idxmax()
    peak_reg_ts = nat_reg.idxmax()
    peak_diff_pct = (peak_reg_val - peak_ref_val) / peak_ref_val * 100 if peak_ref_val else float("nan")
    reg_at_ref_peak = float(nat_reg.loc[peak_ref_ts])
    ref_at_reg_peak = float(nat_ref.loc[peak_reg_ts])

    ref_pct = nat_ref.quantile([p / 100 for p in PCTL])
    reg_pct = nat_reg.quantile([p / 100 for p in PCTL])

    lf_ref = annual_ref / (peak_ref_val * len(nat_ref)) if peak_ref_val else float("nan")
    lf_reg = annual_reg / (peak_reg_val * len(nat_ref)) if peak_reg_val else float("nan")

    nat_mae = float(np.abs(nat_reg - nat_ref).mean())
    nat_nmae = nat_mae / nat_ref.mean() if nat_ref.mean() else float("nan")
    nat_rmse = float(np.sqrt(((nat_reg - nat_ref) ** 2).mean()))

    county_mae = (df_reg - df_ref).abs().mean()
    county_nmae = county_mae.div(df_ref.mean().replace(0, np.nan))
    county_rmse = np.sqrt(((df_reg - df_ref) ** 2).mean())
    mean_rmse = float(county_rmse.mean())
    median_rmse = float(county_rmse.median())
    max_rmse_idx = county_rmse.idxmax()
    min_rmse_idx = county_rmse.idxmin()

    mean_mae = float(county_mae.mean())
    median_mae = float(county_mae.median())
    mean_nmae = float(county_nmae.mean())
    median_nmae = float(county_nmae.median())
    if county_nmae.dropna().empty:
        min_nmae_idx = None
        max_nmae_idx = None
    else:
        min_nmae_idx = county_nmae.idxmin()
        max_nmae_idx = county_nmae.idxmax()

    if not df_ref_ba.empty:
        ba_mae = (df_reg_ba - df_ref_ba).abs().mean()
        ba_nmae = ba_mae.div(df_ref_ba.mean().replace(0, np.nan))
        ba_rmse = np.sqrt(((df_reg_ba - df_ref_ba) ** 2).mean())
        mean_ba_mae = float(ba_mae.mean())
        median_ba_mae = float(ba_mae.median())
        mean_ba_nmae = float(ba_nmae.mean())
        median_ba_nmae = float(ba_nmae.median())
        mean_ba_rmse = float(ba_rmse.mean())
        median_ba_rmse = float(ba_rmse.median())
        if ba_nmae.dropna().empty:
            min_ba_nmae_idx = None
            max_ba_nmae_idx = None
        else:
            min_ba_nmae_idx = ba_nmae.idxmin()
            max_ba_nmae_idx = ba_nmae.idxmax()
        if ba_rmse.dropna().empty:
            ba_max_rmse_idx = None
            ba_min_rmse_idx = None
        else:
            ba_max_rmse_idx = ba_rmse.idxmax()
            ba_min_rmse_idx = ba_rmse.idxmin()
    else:
        ba_mae = pd.Series(dtype=float)
        ba_nmae = pd.Series(dtype=float)
        ba_rmse = pd.Series(dtype=float)
        mean_ba_mae = float("nan")
        median_ba_mae = float("nan")
        mean_ba_nmae = float("nan")
        median_ba_nmae = float("nan")
        mean_ba_rmse = float("nan")
        median_ba_rmse = float("nan")
        min_ba_nmae_idx = None
        max_ba_nmae_idx = None
        ba_max_rmse_idx = None
        ba_min_rmse_idx = None

    county_peak_ref = df_ref.max()
    county_peak_reg = df_reg.max()
    county_peak_pct = (county_peak_reg - county_peak_ref).div(county_peak_ref.replace(0, np.nan)) * 100
    finite_peak = county_peak_pct.dropna()
    peak_mean = float(finite_peak.mean()) if not finite_peak.empty else float("nan")
    peak_median = float(finite_peak.median()) if not finite_peak.empty else float("nan")
    if finite_peak.empty:
        peak_max_idx = None
        peak_min_idx = None
    else:
        peak_max_idx = county_peak_pct.idxmax()
        peak_min_idx = county_peak_pct.idxmin()

    nat_nmae_pct = nat_nmae * 100 if np.isfinite(nat_nmae) else float("nan")

    plot_monthly_pct(monthly_pct, OUTPUT_DIR / "monthly_percent_diff.png")
    plot_top_county_pct(county_pct, OUTPUT_DIR / "county_percent_diff_top.png", label_lookup=fmt_county)
    plot_daily_scatter(daily_ref, daily_reg, OUTPUT_DIR / "daily_totals_scatter.png")
    plot_hourly_scatter(nat_ref, nat_reg, OUTPUT_DIR / "hourly_totals_scatter.png")
    plot_ranked_hourly_scatters(df_ref, df_reg, county_nmae, fmt_county, "county_hourly_scatter")
    if not ba_nmae.empty:
        plot_ranked_hourly_scatters(df_ref_ba, df_reg_ba, ba_nmae, fmt_ba, "ba_hourly_scatter")

    summary_rows = [
        ("Ref rows", f"{len(df_ref_raw):,}"),
        ("Reg rows", f"{len(df_reg_raw):,}"),
        ("Common hours", f"{len(common_idx):,}"),
        ("Missing in reg", f"{len(miss_ref_only):,}"),
        ("Missing in ref", f"{len(miss_reg_only):,}"),
        ("Counties", f"{n:,}"),
        ("Mapping missing counties", f"{len(missing_meta):,}"),
    ]
    if not df_ref_ba.empty:
        summary_rows.append(("Balancing areas", f"{df_ref_ba.shape[1]:,}"))

    seasonal_rows = [
        (k, f"{season_ref[k]:,.2f}", f"{season_reg[k]:,.2f}", fmt_pct(seasonal_pct[k])) for k in SEASON_ORDER
    ]
    monthly_rows = [
        (f"{m:02d}", f"{monthly_nat_ref[m]:,.2f}", f"{monthly_nat_reg[m]:,.2f}", fmt_pct(monthly_pct[m])) for m in range(1, 13)
    ]

    county_stats_items = [
        f"Mean abs pct diff: {mean_abs:.2f}%",
        f"Median abs pct diff: {median_abs:.2f}%",
    ]
    if max_pos is not None and max_neg is not None:
        county_stats_items.append(f"Max positive pct diff: {county_pct[max_pos]:.2f}% ({fmt_county(max_pos)})")
        county_stats_items.append(f"Max negative pct diff: {county_pct[max_neg]:.2f}% ({fmt_county(max_neg)})")
    county_top_pos_rows = [(fmt_county(fips), fmt_pct(pct)) for fips, pct in pos5.items()]
    county_top_neg_rows = [(fmt_county(fips), fmt_pct(pct)) for fips, pct in neg5.items()]

    ba_stats_items = []
    ba_top_pos_rows = []
    ba_top_neg_rows = []
    if not ba_pct.empty:
        ba_stats_items = [
            f"Mean abs pct diff: {ba_mean_abs:.2f}%",
            f"Median abs pct diff: {ba_median_abs:.2f}%",
        ]
        if ba_max_pos is not None and ba_max_neg is not None:
            ba_stats_items.append(f"Max positive pct diff: {ba_pct[ba_max_pos]:.2f}% ({fmt_ba(ba_max_pos)})")
            ba_stats_items.append(f"Max negative pct diff: {ba_pct[ba_max_neg]:.2f}% ({fmt_ba(ba_max_neg)})")
        ba_top_pos_rows = [(fmt_ba(ba), fmt_pct(pct)) for ba, pct in ba_pos5.items()]
        ba_top_neg_rows = [(fmt_ba(ba), fmt_pct(pct)) for ba, pct in ba_neg5.items()]

    mape_rows = [
        ("County-month MAPE (ref>0)", fmt_pct(mean_mape_cm), fmt_pct(median_mape_cm)),
    ]
    if not df_ref_ba.empty:
        mape_rows.append(("BA-month MAPE (ref>0)", fmt_pct(mean_mape_ba), fmt_pct(median_mape_ba)))

    diurnal_rows = [(f"{m:02d}", fmt_float(c, 4)) for m, c in monthly_diurnal_corr]

    national_error_rows = [
        ("MAE", fmt_gwh(nat_mae)),
        ("Normalized MAE", f"{fmt_float(nat_nmae)} ({fmt_pct(nat_nmae_pct)})"),
        ("RMSE", fmt_gwh(nat_rmse)),
    ]

    county_error_rows = [
        ("MAE mean", fmt_gwh(mean_mae)),
        ("MAE median", fmt_gwh(median_mae)),
        ("NMAE mean", fmt_float(mean_nmae)),
        ("NMAE median", fmt_float(median_nmae)),
    ]
    if min_nmae_idx is not None and max_nmae_idx is not None:
        county_error_rows.append(("Best NMAE county", f"{fmt_float(county_nmae[min_nmae_idx])} ({fmt_county(min_nmae_idx)})"))
        county_error_rows.append(("Worst NMAE county", f"{fmt_float(county_nmae[max_nmae_idx])} ({fmt_county(max_nmae_idx)})"))
    county_error_rows.extend(
        [
            ("RMSE mean", fmt_gwh(mean_rmse)),
            ("RMSE median", fmt_gwh(median_rmse)),
            ("Max RMSE county", f"{fmt_gwh(county_rmse[max_rmse_idx])} ({fmt_county(max_rmse_idx)})"),
            ("Min RMSE county", f"{fmt_gwh(county_rmse[min_rmse_idx])} ({fmt_county(min_rmse_idx)})"),
        ]
    )

    ba_error_rows = []
    if not ba_rmse.empty:
        ba_error_rows = [
            ("MAE mean", fmt_gwh(mean_ba_mae)),
            ("MAE median", fmt_gwh(median_ba_mae)),
            ("NMAE mean", fmt_float(mean_ba_nmae)),
            ("NMAE median", fmt_float(median_ba_nmae)),
        ]
        if min_ba_nmae_idx is not None and max_ba_nmae_idx is not None:
            ba_error_rows.append(("Best NMAE BA", f"{fmt_float(ba_nmae[min_ba_nmae_idx])} ({fmt_ba(min_ba_nmae_idx)})"))
            ba_error_rows.append(("Worst NMAE BA", f"{fmt_float(ba_nmae[max_ba_nmae_idx])} ({fmt_ba(max_ba_nmae_idx)})"))
        ba_error_rows.extend(
            [
                ("RMSE mean", fmt_gwh(mean_ba_rmse)),
                ("RMSE median", fmt_gwh(median_ba_rmse)),
            ]
        )
        if ba_max_rmse_idx is not None and ba_min_rmse_idx is not None:
            ba_error_rows.append(("Max RMSE BA", f"{fmt_gwh(ba_rmse[ba_max_rmse_idx])} ({fmt_ba(ba_max_rmse_idx)})"))
            ba_error_rows.append(("Min RMSE BA", f"{fmt_gwh(ba_rmse[ba_min_rmse_idx])} ({fmt_ba(ba_min_rmse_idx)})"))

    peak_rows = [
        ("Ref peak", f"{fmt_gwh(peak_ref_val)} at {peak_ref_ts}"),
        ("Reg peak", f"{fmt_gwh(peak_reg_val)} at {peak_reg_ts}"),
        ("Peak percent diff", fmt_pct(peak_diff_pct)),
        (
            "Reg at ref-peak hour",
            f"{fmt_gwh(reg_at_ref_peak)} (pct diff vs ref {fmt_pct((reg_at_ref_peak - peak_ref_val) / peak_ref_val * 100 if peak_ref_val else float('nan'))})",
        ),
        (
            "Ref at reg-peak hour",
            f"{fmt_gwh(ref_at_reg_peak)} (pct diff vs reg {fmt_pct((ref_at_reg_peak - peak_reg_val) / peak_reg_val * 100 if peak_reg_val else float('nan'))})",
        ),
        ("Load factor ref", fmt_float(lf_ref, 3)),
        ("Load factor reg", fmt_float(lf_reg, 3)),
        ("Load factor delta", fmt_float(lf_reg - lf_ref, 3)),
    ]

    threshold_rows = []
    for p in PCTL:
        pdiff = (reg_pct.loc[p / 100] - ref_pct.loc[p / 100]) / ref_pct.loc[p / 100] * 100
        threshold_rows.append(
            (
                f"{p}%",
                fmt_gwh(ref_pct.loc[p / 100]),
                fmt_gwh(reg_pct.loc[p / 100]),
                fmt_pct(pdiff),
            )
        )

    peak_pct_rows = [
        ("Mean", fmt_pct(peak_mean)),
        ("Median", fmt_pct(peak_median)),
    ]
    if peak_max_idx is not None and peak_min_idx is not None:
        peak_pct_rows.append(("Max", f"{fmt_pct(county_peak_pct[peak_max_idx])} ({fmt_county(peak_max_idx)})"))
        peak_pct_rows.append(("Min", f"{fmt_pct(county_peak_pct[peak_min_idx])} ({fmt_county(peak_min_idx)})"))

    warnings_list = []
    if missing_meta:
        missing_preview = ", ".join(missing_meta[:5])
        suffix = "..." if len(missing_meta) > 5 else ""
        warnings_list.append(f"Counties missing in mapping file: {len(missing_meta)} ({missing_preview}{suffix})")
    if multi_state:
        example_ba = next(iter(multi_state))
        warnings_list.append(f"BAs with multiple states detected (example {example_ba}: {multi_state[example_ba]})")

    charts = [
        ("Monthly percent difference", "monthly_percent_diff.png"),
        ("County percent difference (top +/-)", "county_percent_diff_top.png"),
        ("Daily totals scatter", "daily_totals_scatter.png"),
        ("Hourly totals scatter", "hourly_totals_scatter.png"),
    ]
    county_scatter_imgs = sorted([p.name for p in OUTPUT_DIR.glob("county_hourly_scatter_*.png")])
    ba_scatter_imgs = sorted([p.name for p in OUTPUT_DIR.glob("ba_hourly_scatter_*.png")])

    style = """
    <style>
    body { font-family: Arial, sans-serif; margin: 24px; color: #111; background: #fafafa; }
    h1 { margin-bottom: 6px; }
    h2 { margin-top: 24px; margin-bottom: 8px; }
    table { border-collapse: collapse; margin: 12px 0; width: auto; }
    th, td { border: 1px solid #ccc; padding: 6px 10px; text-align: left; }
    th { background: #f0f0f0; }
    section { margin-bottom: 24px; }
    .grid { display: flex; flex-wrap: wrap; gap: 12px; }
    .grid figure { margin: 0; }
    .grid img { max-width: 360px; border: 1px solid #ddd; background: white; padding: 4px; }
    </style>
    """

    html_parts = [
        "<!DOCTYPE html>",
        "<html><head><meta charset='utf-8'><title>Validation report</title>",
        style,
        "</head><body>",
        "<h1>Validation report</h1>",
        f"<p>Generated from ref: {escape_html(str(REF_PATH))}<br>reg: {escape_html(str(REG_PATH))}</p>",
        "<section><h2>Overview</h2>",
        html_table(summary_rows, ["Metric", "Value"]),
        "</section>",
        "<section><h2>Annual totals</h2>",
        html_table(
            [
                ("Annual ref", f"{annual_ref:,.2f} GWh"),
                ("Annual reg", f"{annual_reg:,.2f} GWh"),
                ("Percent diff (reg-ref)/ref", fmt_pct(annual_pct)),
            ],
            ["Metric", "Value"],
        ),
        "</section>",
        "<section><h2>Seasonal percent difference</h2>",
        html_table(seasonal_rows, ["Season", "Ref (GWh)", "Reg (GWh)", "Pct diff"]),
        "</section>",
        "<section><h2>Monthly percent difference</h2>",
        html_table(monthly_rows, ["Month", "Ref (GWh)", "Reg (GWh)", "Pct diff"]),
        "</section>",
        "<section><h2>County percent difference stats</h2>",
        bullet_list(county_stats_items),
        html_table(county_top_pos_rows, ["Top + counties", "Pct diff"]),
        html_table(county_top_neg_rows, ["Top - counties", "Pct diff"]),
        "</section>",
    ]

    if ba_stats_items:
        html_parts.extend(
            [
                "<section><h2>Balancing area percent difference stats</h2>",
                bullet_list(ba_stats_items),
                html_table(ba_top_pos_rows, ["Top + BAs", "Pct diff"]),
                html_table(ba_top_neg_rows, ["Top - BAs", "Pct diff"]),
                "</section>",
            ]
        )

    html_parts.extend(
        [
            "<section><h2>MAPE</h2>",
            html_table(mape_rows, ["Metric", "Mean", "Median"]),
            "</section>",
            "<section><h2>Correlations</h2>",
            html_table(
                [
                    ("National hourly correlation", fmt_float(hourly_corr, 4)),
                    ("Daily total correlation", fmt_float(daily_corr, 4)),
                ],
                ["Metric", "Value"],
            ),
            "</section>",
            "<section><h2>Diurnal profile correlation by month</h2>",
            html_table(diurnal_rows, ["Month", "Correlation"]),
            "</section>",
            "<section><h2>Error metrics (national)</h2>",
            html_table(national_error_rows, ["Metric", "Value"]),
            "</section>",
            "<section><h2>Error metrics (counties)</h2>",
            html_table(county_error_rows, ["Metric", "Value"]),
            "</section>",
        ]
    )

    if ba_error_rows:
        html_parts.extend(
            [
                "<section><h2>Error metrics (balancing areas)</h2>",
                html_table(ba_error_rows, ["Metric", "Value"]),
                "</section>",
            ]
        )

    html_parts.extend(
        [
            "<section><h2>Peak demand</h2>",
            html_table(peak_rows, ["Metric", "Value"]),
            "</section>",
            "<section><h2>High-load thresholds</h2>",
            html_table(threshold_rows, ["Percentile", "Ref", "Reg", "Pct diff"]),
            "</section>",
            "<section><h2>County peak percent diff</h2>",
            html_table(peak_pct_rows, ["Metric", "Value"]),
            "</section>",
            "<section><h2>Charts</h2>",
            "<div class='grid'>"
            + "".join(
                f"<figure><figcaption>{escape_html(title)}</figcaption><img src='{escape_html(filename)}' alt='{escape_html(title)}'></figure>"
                for title, filename in charts
            )
            + "</div>",
            "</section>",
        ]
    )

    if county_scatter_imgs:
        html_parts.extend(
            [
                "<section><h2>County hourly reg vs ref scatters (ranked by NMAE)</h2>",
                "<div class='grid'>"
                + "".join(
                    f"<figure><figcaption>{escape_html(Path(fname).stem)}</figcaption><img src='{escape_html(fname)}' alt='{escape_html(Path(fname).stem)}'></figure>"
                    for fname in county_scatter_imgs
                )
                + "</div>",
                "</section>",
            ]
        )

    if ba_scatter_imgs:
        html_parts.extend(
            [
                "<section><h2>BA hourly reg vs ref scatters (ranked by NMAE)</h2>",
                "<div class='grid'>"
                + "".join(
                    f"<figure><figcaption>{escape_html(Path(fname).stem)}</figcaption><img src='{escape_html(fname)}' alt='{escape_html(Path(fname).stem)}'></figure>"
                    for fname in ba_scatter_imgs
                )
                + "</div>",
                "</section>",
            ]
        )

    if warnings_list:
        html_parts.extend(
            [
                "<section><h2>Warnings</h2>",
                bullet_list(warnings_list),
                "</section>",
            ]
        )

    html_parts.append("</body></html>")

    report_path = OUTPUT_DIR / "report.html"
    report_path.write_text("\n".join(html_parts), encoding="utf-8")

    try:
        shutil.copy(Path(__file__).resolve(), OUTPUT_DIR / Path(__file__).name)
    except Exception as exc:
        print(f"Warning: failed to copy script into outputs: {exc}")

    print(f"Report written to {report_path.resolve()}")


if __name__ == "__main__":
    main()
